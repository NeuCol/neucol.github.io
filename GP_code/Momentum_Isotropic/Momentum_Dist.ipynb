{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need PyTorch and GPyTorch\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import gpytorch\n",
    "# Gaussian processes really don't work well without 64-bit precision\n",
    "torch.set_default_dtype(torch.float64)\n",
    "# For reproducibility:\n",
    "torch.random.manual_seed(12345)\n",
    "\n",
    "from norm_constrain import Norm2ConstrainedContainer_SE, Norm2ConstrainedContainer_rational, Norm2ConstrainedContainer_ConvexCombination\n",
    "\n",
    "# And a few more players\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data, check normalization, which should be 2 * (2\\pi**2)\n",
    "\n",
    "with open('nk_afdmc.dat', 'r') as fd:\n",
    "    lines = fd.readlines()\n",
    "\n",
    "x = [] ; y = [] ; err = []\n",
    "for line in lines:\n",
    "    buf = line.split()\n",
    "    x.append(float(buf[0]))\n",
    "    y.append(float(buf[1]))\n",
    "    err.append(float(buf[2]))\n",
    "x = np.array(x) ; y = np.array(y) ; err = np.array(err)\n",
    "\n",
    "dx = x[1]-x[0]\n",
    "ig = (y * x**2).sum() *dx / (2 * np.pi**2)\n",
    "print(ig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample training data\n",
    "subrate = 10\n",
    "train_x = torch.tensor(x[::subrate])\n",
    "train_y = torch.tensor(y[::subrate])\n",
    "train_err = torch.tensor(err[::subrate])**2\n",
    "\n",
    "\n",
    "# Normalization value\n",
    "nval = 4 * np.pi**2\n",
    "\n",
    "# Create the GPyTorch model\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, nval):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.nm0 = Norm2ConstrainedContainer_SE(norm_val=nval)\n",
    "        self.nm0.sigma = 2.947 ; self.nm0.gamma = 0.603 ; self.nm0.A = 0.752\n",
    "        self.nm1 = Norm2ConstrainedContainer_rational(norm_val=nval)\n",
    "        self.nm1.alpha = 1.687 ; self.nm1.p = 10.155 ; self.nm1.A = 1.400\n",
    "        self.nm2 = Norm2ConstrainedContainer_SE(norm_val=nval)\n",
    "        self.mod = Norm2ConstrainedContainer_ConvexCombination(norm_val=nval, kernels=(self.nm0, self.nm1,self.nm2))\n",
    "        self.mod.compositions = torch.tensor([0.003, 0.996])\n",
    "        # Using the normalization-constrained model\n",
    "        self.covar_module = self.mod.covar_module\n",
    "        self.mean_module = self.mod.mean_module\n",
    "        \n",
    " \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=train_err)\n",
    "\n",
    "model = GPRegressionModel(train_x, train_y, likelihood, nval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "training_iter = 1000\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f\\nCompositions: %s\\nSE: sigma: %.3f  gamma: %.3f  A: %.3f\\nRational 1: alpha: %.3f  p: %.3f  A: %.3f\\nSE 2: sigma: %.3f  gamma: %.3f  A: %.3f\\n' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.mod.full_compositions.tolist(),\n",
    "        model.mod.kernels[0].sigma.item(),\n",
    "        model.mod.kernels[0].gamma.item(),\n",
    "        model.mod.kernels[0].A.item(),\n",
    "        model.mod.kernels[1].alpha.item(),\n",
    "        model.mod.kernels[1].p.item(),\n",
    "        model.mod.kernels[1].A.item(),\n",
    "        model.mod.kernels[2].sigma.item(),\n",
    "        model.mod.kernels[2].gamma.item(),\n",
    "        model.mod.kernels[2].A.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final state:\")\n",
    "print('Loss: %.3f\\nCompositions: %s\\nSE: sigma: %.3f  gamma: %.3f  A: %.3f\\nRational 1: alpha: %.3f  p: %.3f  A: %.3f\\nSE 2: sigma: %.3f  gamma: %.3f  A: %.3f\\n' % (\n",
    "        loss.item(),\n",
    "        model.mod.full_compositions.tolist(),\n",
    "        model.mod.kernels[0].sigma.item(),\n",
    "        model.mod.kernels[0].gamma.item(),\n",
    "        model.mod.kernels[0].A.item(),\n",
    "        model.mod.kernels[1].alpha.item(),\n",
    "        model.mod.kernels[1].p.item(),\n",
    "        model.mod.kernels[1].A.item(),\n",
    "        model.mod.kernels[2].sigma.item(),\n",
    "        model.mod.kernels[2].gamma.item(),\n",
    "        model.mod.kernels[2].A.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction time\n",
    "N_Pred = 1000\n",
    "x_max = x[-1]*1.05\n",
    "pred_x = torch.linspace(0.0, x_max, N_Pred)\n",
    "ind_t = np.arange(x.shape[0],dtype=int)\n",
    "ind_t = ind_t[ind_t % subrate != 0]\n",
    "test_x = x[ind_t]\n",
    "test_y = y[ind_t]\n",
    "test_err = err[ind_t]\n",
    "\n",
    "tx = train_x.numpy().flatten()\n",
    "ty = train_y.detach().numpy()\n",
    "terr = train_err.sqrt().numpy()\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad():\n",
    "#    preds = likelihood(model(pred_x))\n",
    "    preds = model(pred_x)\n",
    "\n",
    "pred_x = pred_x.numpy()\n",
    "lower, upper = preds.confidence_region() # 2-sigma confidence region\n",
    "pm = preds.mean.numpy()\n",
    "upp = upper.numpy()\n",
    "low = lower.numpy()\n",
    "\n",
    "def plotit(filename, xmin=None, xmax=None, ymin=None, ymax=None):\n",
    "\n",
    "    fig1 = plt.figure()\n",
    "    fig1.set_figwidth(16.0)\n",
    "    fig1.set_figheight(8.0)\n",
    "    ax1 = fig1.add_subplot(1,2,1)\n",
    "\n",
    "    ax1.plot(pred_x, pm, \"r-\",label=\"Mean prediction\")\n",
    "    #ax1.fill_between(pred_x, low, upp, alpha = 0.3, label=r\"2-$\\sigma$ uncertainty\")\n",
    "\n",
    "    ax1.errorbar(test_x, test_y, yerr=test_err, ecolor='b', marker='o', ms=8, mfc='blue', mec='blue', label=\"Held out data\",ls='None')\n",
    "    ax1.errorbar(tx,ty, yerr=terr, ecolor='y', marker='v', ms=12, mfc='y', mec='y', ls='None', label=\"Training data\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax1.tick_params(labelsize=20)\n",
    "    ax1.set_xlabel('X', size=20)\n",
    "    ax1.set_ylabel(r'Y', size=20)\n",
    "    ax1.set_ylim(ymin=ymin, ymax=ymax)\n",
    "    ax1.set_xlim(xmin=xmin, xmax=xmax)\n",
    "\n",
    "    ax2 = fig1.add_subplot(1,2,2)\n",
    "\n",
    "    ax2.plot(pred_x, pred_x**2 *pm, \"r-\",label=\"Mean prediction\")\n",
    "    #ax2.fill_between(pred_x, pred_x**2 * low, pred_x**2 * upp, alpha = 0.3, label=r\"2-$\\sigma$ uncertainty\")\n",
    "\n",
    "    ax2.errorbar(test_x, test_x**2 * test_y, yerr=test_x**2 * test_err, ecolor='b', marker='o', ms=8, mfc='blue', mec='blue', label=\"Held out data\",ls='None')\n",
    "    ax2.errorbar(tx,tx**2 * ty, yerr=tx**2 * terr, ecolor='y', marker='v', ms=12, mfc='y', mec=\"y\", ls='None', label=\"Training data\")\n",
    "\n",
    "    ax2.legend()\n",
    "\n",
    "    ax2.tick_params(labelsize=20)\n",
    "    ax2.set_xlabel('X', size=20)\n",
    "    ax2.set_ylabel(r'Y$\\times$X$^2$', size=20)\n",
    "\n",
    "    ax2.set_ylim(ymin=ymin, ymax=ymax)\n",
    "    ax2.set_xlim(xmin=xmin, xmax=xmax)\n",
    "\n",
    "    plt.savefig(filename, format=\"png\")\n",
    "\n",
    "plotit(\"gp_momentum_emulator.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in\n",
    "plotit(\"gp_momentum_emulator_zoom.png\", xmax=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mean function normalization\n",
    "def integrand(x):\n",
    "    z = torch.tensor([x])\n",
    "    with torch.no_grad():\n",
    "        pred = model(z) / (2 * np.pi**2)\n",
    "    mean = pred.mean * z**2\n",
    "    return mean.detach().numpy()\n",
    "\n",
    "quad_res = quad(integrand, 0.0, np.inf)\n",
    "print(\"Quadrature integral = %.9f, error = %10.3E\" % quad_res)\n",
    "\n",
    "# Get the mse of held-out values\n",
    "Test_x = torch.tensor(test_x) ; Test_y = torch.tensor(test_y)\n",
    "with torch.no_grad():\n",
    "    pred = model(Test_x)\n",
    "mean = pred.mean\n",
    "mse = ((pred.mean - test_y)**2).mean().sqrt()\n",
    "nrm = (pred.mean**2).mean().sqrt()\n",
    "mse = mse / nrm\n",
    "print(\"Normalized MSE of held-out data = %12.5E\" % mse)\n",
    "\n",
    "# Largest relative error\n",
    "relerr = (pred.mean - test_y).abs() / pred.mean.abs()\n",
    "relerr_max, ind = relerr.max(dim=0)\n",
    "print(\"Largest relative error = %12.5E at x = %12.5E\" % (relerr_max, Test_x[ind]))\n",
    "print(\"Mean relative error = %12.5E\" % relerr.mean())\n",
    "\n",
    "# Restrict to x < xmax, repeat error analysis\n",
    "xmax = 2.5\n",
    "Tx2 = Test_x[Test_x < xmax] ; Ty2 = Test_y[Test_x<xmax] ; mn2 = pred.mean[Test_x<xmax]\n",
    "mse = ((mn2 - Ty2)**2).mean().sqrt()\n",
    "nrm = (mn2**2).mean().sqrt()\n",
    "mse = mse / nrm\n",
    "relerr = (mn2 - Ty2).abs() / mn2.abs()\n",
    "relerr_max, ind = relerr.max(dim=0)\n",
    "print(\"\\nRestricted to x < %f:\" % xmax)\n",
    "print(\"Normalized MSE of held-out data = %12.5E\" % mse)\n",
    "print(\"Largest relative error = %12.5E at x = %12.5E\" % (relerr_max, Tx2[ind]))\n",
    "print(\"Mean relative error = %12.5E\" % relerr.mean())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
